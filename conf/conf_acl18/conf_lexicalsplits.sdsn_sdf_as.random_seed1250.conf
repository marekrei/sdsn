[config]
dataset_mode = lexical
features_type = .simdepwin
path_train = data/hyperlex/splits/lexical/hyperlex_training_all_lexical.cleaned.simdepwin.tsv
path_dev = data/hyperlex/splits/lexical/hyperlex_dev_all_lexical.cleaned.simdepwin.tsv
path_test = data/hyperlex/splits/lexical/hyperlex_test_all_lexical.cleaned.simdepwin.tsv
path_pretrain = data/PPDB_synonyms/equivalence.simdepwin.rescaled.tsv,data/PPDB_synonyms/exclusion.simdepwin.rescaled.tsv,data/HyponymGen/detection/hyponym-detection-all.positive.simdepwin.rescaled.tsv,data/HyponymGen/detection/hyponym-detection-all.negative.simdepwin.rescaled.tsv
embedding_type = levy-dependency
word_embedding_path_a = embeddings/levy-dependency/deps.words
word_embedding_size_a = 300
word_embedding_path_b = 
word_embedding_size_b = 0
stop_if_no_improvement_for_epochs = 7
examples_per_batch = 32
shuffle_training_data = True
gating = both
embedding_mapping_size = 300
embedding_combination = multiply
embedding_dropout = 0.5
combination_layer_dropout = 0.0
hidden_layer_dropout = 0.0
tanh_location = before
late_fusion = False
cost = mse
output_method = scaled_sigmoid
update_embeddings = False
extend_vocabulary = False
cost_l2 = 0.0
optimisation_strategy = adadelta
learningrate = 1.0
hidden_layer_size = 100
save = 
load = 
epochs = 200
model_selector = dev_spearmanr
restrict_to_embedded_vocab = True
init_embeddings_zero = True
pretrain_epochs = 1
pretrain_margin = 1.0
random_seed = 1250
