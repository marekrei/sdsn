[config]
dataset_mode = random
features_type = 
path_train = data/hyperlex/splits/random/hyperlex_training_all_random.cleaned.tsv
path_dev = data/hyperlex/splits/random/hyperlex_dev_all_random.cleaned.tsv
path_test = data/hyperlex/splits/random/hyperlex_test_all_random.cleaned.tsv
path_pretrain = 
embedding_type = levy-dependency
word_embedding_path_a = embeddings/levy-dependency/deps.words
word_embedding_size_a = 300
word_embedding_path_b = 
word_embedding_size_b = 0
stop_if_no_improvement_for_epochs = 7
examples_per_batch = 32
shuffle_training_data = True
gating = both
embedding_mapping_size = 300
embedding_combination = multiply
embedding_dropout = 0.5
combination_layer_dropout = 0.0
hidden_layer_dropout = 0.0
tanh_location = before
late_fusion = False
cost = mse
output_method = scaled_sigmoid
update_embeddings = False
extend_vocabulary = False
cost_l2 = 0.0
optimisation_strategy = adadelta
learningrate = 1.0
hidden_layer_size = 100
save = 
load = 
epochs = 200
model_selector = dev_spearmanr
restrict_to_embedded_vocab = True
init_embeddings_zero = True
pretrain_epochs = 0
pretrain_margin = 0.0
random_seed = 1250
